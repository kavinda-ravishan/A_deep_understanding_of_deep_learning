{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (1): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): Linear(in_features=2, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "widenet = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.Linear(4, 3)\n",
    ")\n",
    "\n",
    "deepnet = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Linear(2, 3)\n",
    ")\n",
    "\n",
    "print(widenet, end='\\n\\n')\n",
    "print(deepnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', Parameter containing:\n",
      "tensor([[ 0.2695,  0.5613],\n",
      "        [-0.0740,  0.6155]], requires_grad=True))\n",
      "\n",
      "('0.bias', Parameter containing:\n",
      "tensor([ 0.2429, -0.0915], requires_grad=True))\n",
      "\n",
      "('1.weight', Parameter containing:\n",
      "tensor([[ 0.6339, -0.0241],\n",
      "        [-0.1968,  0.3195]], requires_grad=True))\n",
      "\n",
      "('1.bias', Parameter containing:\n",
      "tensor([-0.4535, -0.6919], requires_grad=True))\n",
      "\n",
      "('2.weight', Parameter containing:\n",
      "tensor([[-0.5542,  0.0681],\n",
      "        [-0.3084, -0.2567],\n",
      "        [ 0.5060, -0.6863]], requires_grad=True))\n",
      "\n",
      "('2.bias', Parameter containing:\n",
      "tensor([ 0.2320,  0.6932, -0.1243], requires_grad=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in deepnet.named_parameters():\n",
    "    print(p, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 nodes in the wide network.\n",
      "There are 7 nodes in the deep network.\n"
     ]
    }
   ],
   "source": [
    "numNodeInWide = 0\n",
    "for p in widenet.named_parameters():\n",
    "\n",
    "    if 'bias' in p[0]:\n",
    "        numNodeInWide += len(p[1])\n",
    "\n",
    "numNodeInDeep = 0\n",
    "for paramName, paramVect in widenet.named_parameters():\n",
    "\n",
    "    if 'bias' in paramName:\n",
    "        numNodeInDeep += len(paramVect)\n",
    "\n",
    "print('There are %s nodes in the wide network.' %numNodeInWide)\n",
    "print('There are %s nodes in the deep network.' %numNodeInDeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2695,  0.5613],\n",
      "        [-0.0740,  0.6155]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.2429, -0.0915], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.6339, -0.0241],\n",
      "        [-0.1968,  0.3195]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([-0.4535, -0.6919], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.5542,  0.0681],\n",
      "        [-0.3084, -0.2567],\n",
      "        [ 0.5060, -0.6863]], requires_grad=True)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.2320,  0.6932, -0.1243], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in deepnet.parameters():\n",
    "    print(p, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This piece has 8 parameters.\n",
      "This piece has 4 parameters.\n",
      "This piece has 12 parameters.\n",
      "This piece has 3 parameters.\n",
      "\n",
      "\n",
      "Total of 27 parameters\n"
     ]
    }
   ],
   "source": [
    "# Number of trainable parameters\n",
    "\n",
    "nparams = 0\n",
    "\n",
    "for p in widenet.parameters():\n",
    "\n",
    "    if p.requires_grad:\n",
    "        print('This piece has %s parameters.'%p.numel())\n",
    "        nparams += p.numel()\n",
    "\n",
    "print('\\n\\nTotal of %s parameters'%nparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widenet has 27 parameter.\n",
      "Deepnet has 21 parameter.\n"
     ]
    }
   ],
   "source": [
    "nparams = sum([p.numel() for p in widenet.parameters() if p.requires_grad ])\n",
    "print('Widenet has %s parameter.'%nparams)\n",
    "\n",
    "nparams = sum([p.numel() for p in deepnet.parameters() if p.requires_grad ])\n",
    "print('Deepnet has %s parameter.'%nparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, device, dtypes)\n",
    "    print(result)\n",
    "\n",
    "    return params_info\n",
    "\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, (total_params, trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1, 4]              12\n",
      "            Linear-2                 [-1, 1, 3]              15\n",
      "================================================================\n",
      "Total params: 27\n",
      "Trainable params: 27\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(27), tensor(27))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(widenet, (1, 2), device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "293c111297a61481508202fcd690d673b0775ece2d2d867b62b8842b676a9a30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
